import os
import json
import asyncio
import requests
import streamlit as st
import xml.etree.ElementTree as ET
import chromadb
import ollama
import re
from urllib.parse import urlparse
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from crawl4ai.content_filter_strategy import PruningContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from sentence_transformers import SentenceTransformer

# ğŸ”¹ ConfiguraÃ§Ã£o global
PASTA_JSON = "./data"
CHROMA_DB_PATH = "./chroma_db"
SITEMAP_URL = "https://www.lance.com.br/sitemap/news/today.xml"
MODEL_NAME = "deepseek-r1:8b"
MODEL_EMBEDDING = "all-MiniLM-L6-v2"
MAX_CRAWLERS = 5 

# ğŸ”¹ Conectar ao banco ChromaDB
db = chromadb.PersistentClient(path=CHROMA_DB_PATH)
collection = db.get_or_create_collection(name="noticias")
modelo = SentenceTransformer(MODEL_EMBEDDING)

# ğŸ”¹ FunÃ§Ã£o para extrair slug da URL
def extrair_slug(url):
    path = urlparse(url).path
    slug = path.rstrip('/').split('/')[-1]
    return os.path.splitext(slug)[0]

# ğŸ”¹ FunÃ§Ã£o para extrair links do sitemap
def extrair_links_sitemap(url_sitemap):
    resposta = requests.get(url_sitemap)
    if resposta.status_code != 200:
        st.error(f"Erro ao acessar {url_sitemap}")
        return []
    
    root = ET.fromstring(resposta.text)
    urls = [elem.text for elem in root.findall(".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc")]
    return urls

# ğŸ”¹ FunÃ§Ã£o para rodar um crawler em paralelo
async def rodar_crawler_para_link(link):
    prune_filter = PruningContentFilter(threshold=0.45, threshold_type="dynamic", min_word_threshold=5)
    md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)
    config = CrawlerRunConfig(markdown_generator=md_generator, cache_mode=CacheMode.BYPASS, excluded_tags=["nav", "footer", "header"], exclude_external_links=True)

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=link, config=config)
        if result.success:
            mkResult = result.markdown_v2.fit_markdown
            match = re.search(r'^# (.+)', mkResult, re.MULTILINE)
            title = match.group(1) if match else "Sem tÃ­tulo"

            json_data = {"title": title, "link": link, "content": mkResult}
            filename = os.path.join(PASTA_JSON, f"{extrair_slug(link)}.json")
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(json_data, f, ensure_ascii=False, indent=4)

            return f"âœ… NotÃ­cia salva: {title}"
        else:
            return f"âŒ Erro ao extrair {link}: {result.error_message}"


async def rodar_crawler():
    
    os.makedirs(PASTA_JSON, exist_ok=True)
    links = extrair_links_sitemap(SITEMAP_URL)
    
    # Executar crawlers em paralelo com limite mÃ¡ximo de processos simultÃ¢neos
    resultados = []
    for i in range(0, len(links), MAX_CRAWLERS):
        batch = links[i:i+MAX_CRAWLERS]  # Divide os links em lotes de MAX_CRAWLERS
        resultados += await asyncio.gather(*(rodar_crawler_para_link(link) for link in batch))
    
    for resultado in resultados:
        st.write(resultado)

# ğŸ”¹ FunÃ§Ã£o para carregar arquivos JSON e inserir no ChromaDB
def processar_arquivos_json():
    documentos = []
    for arquivo in os.listdir(PASTA_JSON):
        if arquivo.endswith(".json"):
            caminho_arquivo = os.path.join(PASTA_JSON, arquivo)
            with open(caminho_arquivo, "r", encoding="utf-8") as f:
                dados = json.load(f)

            slug = os.path.splitext(arquivo)[0]
            documentos.append({"id": slug, "title": dados.get("title", slug), "link": dados.get("link", ""), "content": dados.get("content", "")})

    for doc in documentos:
        embedding = modelo.encode(doc["content"]).tolist()
        collection.add(ids=[doc["id"]], embeddings=[embedding], metadatas=[{"title": doc["title"], "link": doc["link"], "content": doc["content"]}])

    st.success(f"{len(documentos)} documentos inseridos no ChromaDB!")


def apagar_arquivos_json():
    """Apaga todos os arquivos JSON dentro do diretÃ³rio de dados."""
    if not os.path.exists(PASTA_JSON):
        st.warning("A pasta de dados nÃ£o existe.")
        return
    
    arquivos = [f for f in os.listdir(PASTA_JSON) if f.endswith(".json")]
    
    if not arquivos:
        st.warning("Nenhum arquivo JSON encontrado para deletar.")
        return

    for arquivo in arquivos:
        caminho_arquivo = os.path.join(PASTA_JSON, arquivo)
        try:
            os.remove(caminho_arquivo)
        except Exception as e:
            st.error(f"Erro ao deletar {arquivo}: {e}")

    st.success(f"ğŸ—‘ï¸ {len(arquivos)} arquivos JSON foram deletados com sucesso!")


# ğŸ”¹ FunÃ§Ã£o para buscar contexto no ChromaDB
def buscar_contexto(consulta):
    consulta_embedding = modelo.encode(consulta).tolist()
    resultados = collection.query(query_embeddings=[consulta_embedding], n_results=3)
    
    contexto = ""
    for r in resultados["metadatas"][0]:
        contexto += f"\nğŸ”¹ {r['title']}\nğŸ“„ {r['content'][:500]}...\nğŸ”— {r['link']}\n\n"

    return contexto.strip() if contexto else "Nenhuma informaÃ§Ã£o relevante encontrada."

# ğŸ”¹ FunÃ§Ã£o para consultar o LLM com contexto
def perguntar_ao_llm(consulta):
    contexto = buscar_contexto(consulta)
    prompt = f"""VocÃª Ã© um assistente especializado em notÃ­cias. 
    Use as informaÃ§Ãµes abaixo para responder Ã  pergunta do usuÃ¡rio, incluindo os links das fontes:

    {contexto}

    Pergunta: {consulta}
    Resposta:"""

    resposta = ollama.chat(model=MODEL_NAME, messages=[{"role": "user", "content": prompt}])
    return resposta['message']['content']

# ğŸ”¹ Interface do Streamlit
st.set_page_config(page_title="Lance! RAG AI", page_icon="ğŸ¤–")

# ğŸ”¹ Criar abas no Streamlit
aba = st.sidebar.radio("NavegaÃ§Ã£o", ["ğŸ“¡ Crawler", "ğŸ“¥ VetorizaÃ§Ã£o", "ğŸ—‘ï¸ Apagar Local",  "ğŸ’¬ Chat"])

if aba == "ğŸ“¡ Crawler":
    st.title("ğŸ“¡ Crawler - ExtraÃ§Ã£o de NotÃ­cias")
    st.write("Clique no botÃ£o abaixo para iniciar a coleta de notÃ­cias do site Lance!")

    if st.button("Iniciar Crawler"):
        st.warning("O processo pode levar alguns minutos...")
        asyncio.run(rodar_crawler())

elif aba == "ğŸ“¥ VetorizaÃ§Ã£o":
    st.title("ğŸ“¥ VetorizaÃ§Ã£o - Inserir NotÃ­cias no ChromaDB")
    st.write("Processar os arquivos JSON e armazenar as notÃ­cias no banco vetorial.")

    if st.button("Processar Arquivos"):
        processar_arquivos_json()

elif aba == "ğŸ—‘ï¸ Apagar Local":
    st.title("ğŸ—‘ï¸ Apagar Arquivos JSON")
    st.write("Clique no botÃ£o abaixo para apagar todos os arquivos JSON extraÃ­dos.")

    if st.button("Apagar Arquivos JSON"):
        apagar_arquivos_json()

elif aba == "ğŸ’¬ Chat":
    st.title("ğŸ¤– LANCE AI")
    st.write("Digite uma pergunta e obtenha respostas baseadas em notÃ­cias armazenadas no banco vetorial.")

    consulta_usuario = st.text_input("Pergunte:", placeholder="Quem joga hoje?")
    
    if st.button("Enviar"):
        if consulta_usuario:
            with st.spinner("ğŸ” Buscando resposta..."):
                resposta = perguntar_ao_llm(consulta_usuario)
            
            st.subheader("ğŸ§  Resposta do LLM:")
            st.write(resposta)
        else:
            st.warning("Por favor, digite uma pergunta antes de continuar.")

# ğŸ”¹ RodapÃ©
st.markdown("---")
st.markdown("âš¡ Desenvolvido com **Ollama + Crawl4AI + ChromaDB + Streamlit + DeepSeek-R1:8B**")
